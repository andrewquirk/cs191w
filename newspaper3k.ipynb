{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewquirk/anaconda3/envs/nlu/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "#import newspaper\n",
    "#from keras.models import Sequential\n",
    "#import keras\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from gensim.models import word2vec\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cosine, euclidean, jaccard\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_colwidth = 200\n",
    "from string import punctuation\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['http://foxnews.com']#, \n",
    "#         'http://breitbart.com', \n",
    "#         'http://economist.com', \n",
    "#         'http://newyorktimes.com',\n",
    "#         'http://www.wsj.com',\n",
    "#         'http://www.huffingtonpost.com',\n",
    "#         'http://www.motherjones.com',\n",
    "#         'http://www.newyorker.com',\n",
    "#         'http://reuters.com',\n",
    "#         'http://usatoday.com',\n",
    "#         'http://npr.org',\n",
    "#         'http://ap.org',\n",
    "#         'http://occupydemocrats.com',\n",
    "#         'http://abcnews.com',\n",
    "#         'http://msnbc.com']\n",
    "\n",
    "from newspaper import Article\n",
    "import newspaper\n",
    "from numpy import prod\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url http://foxnews.com has this many articles: 444\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://nation.foxnews.com/article/5bb2b1c05245a00019050118/ on URL https://nation.foxnews.com/article/5bb2b1c05245a00019050118/\n",
      "beginning training\n"
     ]
    }
   ],
   "source": [
    "for url in urls: \n",
    "    neg = 0.0\n",
    "    pos = 0.0\n",
    "    news_source = newspaper.build(url, memoize_articles=False)\n",
    "    counter = 0.0\n",
    "    print(\"url \" + str(url) + \" has this many articles: \" + str(len(news_source.articles)))\n",
    "\n",
    "    if (len(news_source.articles) > 50):\n",
    "\n",
    "        feature_size = 300   # Word vector dimensionality  \n",
    "        window_context = 30    # Context window size                                                                                    \n",
    "        min_word_count = 1   # Minimum word count                        \n",
    "        sample = 1e-3    # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "        for article in news_source.articles:\n",
    "            ##loading article\n",
    "            urll = article.url\n",
    "            #print(urll)\n",
    "            art = Article(urll)\n",
    "            art.download()\n",
    "            art.parse()\n",
    "            #print(art.text)\n",
    "            unclean_text = art.text\n",
    "        # #     ref_bool = False\n",
    "            tokens.append(nltk.word_tokenize(unclean_text))\n",
    "        print(\"beginning training\")\n",
    "        model = word2vec.Word2Vec(tokens, size=feature_size, window=window_context, min_count=min_word_count, sample=sample, iter=50)\n",
    "        \n",
    "        #print(model.most_similar('good', 10))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prominence', 0.6199158430099487),\n",
       " ('end-of-life-care', 0.6181966066360474),\n",
       " ('abuses', 0.5624952912330627),\n",
       " ('suggest', 0.5571990609169006),\n",
       " ('fraught', 0.5570160150527954),\n",
       " ('conflicts', 0.5512677431106567),\n",
       " ('experimental', 0.5506510734558105),\n",
       " ('pose', 0.5371493101119995),\n",
       " ('Alliance', 0.534617006778717),\n",
       " ('assisted', 0.5304291844367981)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"human\"\n",
    "model.wv.most_similar(positive = w1)\n",
    "vec1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
